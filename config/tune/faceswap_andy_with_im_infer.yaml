# CUDA_VISIBLE_DEVICES=3 python train_tune_a_video.py --config config/tune/jeep.yaml
# There is no obvious difference between v1-4 and v1-4, we just choose to tune v1-5 randomly at the beginning of this project.
pretrained_model_path: "result/tune/faceswap_andy_with_im_240902-222104/checkpoint_8000"

dataset_config:
    path: "data/shape/faceswap_andy"
    prompt: "a photo of a human face"
    image: "data/shape/faceswap_andy/0.png"
    n_sample_frame: 8
    # n_sample_frame: 22
    # class_data_root: "data/negative_reg/car"   # change later
    # class_data_prompt: "a photo of a man"

    sampling_rate: 1
    stride: 80
    offset: 
        left: 0
        right: 0
        top: 0
        bottom: 0

editing_config:
    use_invertion_latents: True
    use_inversion_attention: True
    guidance_scale: 7.5
    editing_prompts: [
        a photo of man speaking with microphone,
        a photo of elun musk face,
        a sketch of a barak obama face,

    ]
    editing_images: [
        data/shape/faceswap_andy/0.png,
        /share/data/drive_3/Sanoojan/needed/Paint_for_swap/examples/Thesis_check_4/Outs/source_cropped/12.png,
        /share/data/drive_3/Sanoojan/needed/Paint_for_swap/examples/Thesis_check_4/Outs/source_cropped/11.png,
    ]
    clip_length: "${..dataset_config.n_sample_frame}"
    sample_seeds: [12734]
    
    num_inference_steps: 50 # 15 minutes
    strength: 0.99

trainer_pipeline_config:
    target: video_diffusion.trainer.ddpm_trainer_image_cond.DDPMTrainer

test_pipeline_config:
    target: video_diffusion.pipelines.ddim_spatial_temporal_image_cond.DDIMSpatioTemporalStableDiffusionPipeline

model_config:
    lora: 160
    # temporal_downsample_time: 4
    # SparseCausalAttention_index: [-1, 1, 'first', 'last'] 

enable_xformers: True
mixed_precision: 'fp16'
gradient_checkpointing: True

train_steps: 10000
validation_steps: 500
checkpointing_steps: 1000
seed: 74831
learning_rate: 5e-6
# prior_preservation: 1.0
train_temporal_conv: True